<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>autograph</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="what-is-machine-learning.html"><strong aria-hidden="true">1.1.</strong> What is Machine Learning?</a></li><li class="chapter-item expanded "><a href="what-is-a-neural-network.html"><strong aria-hidden="true">1.2.</strong> What is a Neural Network?</a></li></ol></li><li class="chapter-item expanded "><a href="device.html"><strong aria-hidden="true">2.</strong> Device</a></li><li class="chapter-item expanded "><a href="num.html"><strong aria-hidden="true">3.</strong> Num</a></li><li class="chapter-item expanded "><a href="tensor-base.html"><strong aria-hidden="true">4.</strong> TensorBase</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tensor.html"><strong aria-hidden="true">4.1.</strong> Tensor</a></li><li class="chapter-item expanded "><a href="tensor-view.html"><strong aria-hidden="true">4.2.</strong> TensorView</a></li><li class="chapter-item expanded "><a href="arc-tensor.html"><strong aria-hidden="true">4.3.</strong> ArcTensor</a></li><li class="chapter-item expanded "><a href="rw-tensor.html"><strong aria-hidden="true">4.4.</strong> RwTensor</a></li></ol></li><li class="chapter-item expanded "><a href="nn.html"><strong aria-hidden="true">5.</strong> nn</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="layer.html"><strong aria-hidden="true">5.1.</strong> Layer</a></li><li class="chapter-item expanded "><a href="forward.html"><strong aria-hidden="true">5.2.</strong> Forward</a></li><li class="chapter-item expanded "><a href="autograd.html"><strong aria-hidden="true">5.3.</strong> autograd</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">autograph</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>The goal of autograph is to provide a Rust platform for Machine Learning. It taps into highly optimized, hardware specific libraries (written in c / c++ / assembly), but attempts to remain as agnostic as possible in regards to implementation. Operations can be implemented in Rust, c++, or any other language without direct dependence on each other, making it highly modular.<br />
Unlike many Machine Learning libraries, autograph is imperative (the natural programming paradigm). This is similar to PyTorch, where operations are called and executed immediately. One alternative is to use a symbolic approach, constructing a plan and repeatedly running it over and over with different data. While the later offers theoretical optimization benefits, it is more restrictive, less intuitive, and may make troublshooting more difficult. An imperative approach allows the insertion of arbitrary print statements for debugging, allows for loops, conditionals, and any other native operations.<br />
One idea at the core of Rust is mutability. In most c based languages, variables are mutable by default. In Rust, variables are immutable by default, and much of the language is designed around providng statically checked means of borrowing data (mutably or immutably) between functions. Inspired by the amazing ndarray crate, autograph emulates ArrayBase with TensorBase. Operations explicitly require immutable access to their inputs and mutable access to their outputs. For shared data, there is ArcTensor, an immutable shared tensor used for Inputs / Outputs, and RwTensor, which uses a RwLock to ensure thread safe shared mutability, used for Parameters and Gradients.<br />
In general, structures in autograph impl Send + Sync, and can be passed safely between threads. Underneath, there is plenty of unsafe used to wrap backend implementations, but the library can be used without writing unsafe code. </p>
<h1><a class="header" href="#what-is-machine-learning" id="what-is-machine-learning">What is Machine Learning?</a></h1>
<p>Machine Learning refers to algorithms that solve problems by iteratively improving until they converge to a solution. Unlike handcrafted logic, such algorithms scale to &quot;Big Data&quot;, potentially with billions of individually tuned parameters. Typically, such models separate datapoints into several classes, known as classification. This may be supervised, which means that some of the datapoints have been labeled by hand, or unsupervised, where the algorithm is not provided labeled datapoints. </p>
<h1><a class="header" href="#what-is-a-neural-network" id="what-is-a-neural-network">What is a Neural Network?</a></h1>
<p>Often referred to as &quot;Deep Learning&quot;, Neural Networks are composed of one or more layers of &quot;Neurons&quot;, that map some input x to some output y. Typically this is 'y = a(x*w + b)', where y, x, and w are matrices, b is a vector, and a is an &quot;Activation Function&quot;. An activation function is nonlinear, this helps the model approximate the dataset. </p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png?raw=true" alt="" /><br />
<em><a href="https://commons.wikimedia.org/wiki/File:Artificial_neural_network.svg">Artificial_neural_network</a></em></p>
<p>In the above image each vertical column of neurons represents a single 'y = a(x*w + b)', where there is a weight entry in w for each input entry in x. A neuron can be thought of a weighted some of the inputs plus a bias and mapped with an activation. The actual implementation uses matrices, with a batch of inputs, which is more efficient. A Neural Network is a supervised algorithm, it is trained by providing it with a target (ie the correct output). A loss function computes the error of the model output agaist the target. The goal of the model is to minimize this loss, which corresponds to approximating the dataset. In order to iteratively improve the prediction y, the parameters (w and b) are updated. This is done via &quot;Gradient Descent&quot;. All functions used to compute the loss are differentiable with respect to either their inputs and or their parameters. These gradients can be propagated backward, from the loss to the parameters. The parameters and then updated with some form of 'w -= lr * dw', where lr is the &quot;learning_rate&quot;, and dw is the gradient of the weight. The same is applied to the bias. </p>
<p>Prior to training, the weights of the model are initialized, perhaps from a normal distribution, while the biases are typically initialized as zeros. Input data is batched and fed into the model as a matrix, or a higher dimensional tensor (in the case of images). Larger batches are more efficient to compute than individual samples. The &quot;Forward Pass&quot; is computing the prediction of the model y and the loss. The &quot;Backward Pass&quot; is the computation of the gradients of the parameters. For each forward operation, there are one or more backward ops that compute the gradients of the inputs / parameters given the gradient of the output. This is the backward graph. Typically the backward ops are executed in First In Last Out order, ie in reverse. The forward, backward, and update steps are repeated for all of the training batches, this is an epoch. Training typically requires many epochs for model to converge to an accurate solution. </p>
<h1><a class="header" href="#device" id="device">Device</a></h1>
<p>A Device represents either a cpu or gpu. Generally the gpu has its own memory. Transferring data from cpu to gpu memory is relatively expensive compared to executing operations. Generally operations can only be performed on data that is in a particular device's memory, at least multi device operations are slower than single device operations. </p>
<p>In autograph, a Device is an enum:</p>
<pre><code>#[derive(Clone, Debug)]
pub enum Device {
    Cpu(Arc&lt;Cpu&gt;),
    #[cfg(feature = &quot;cuda&quot;)]
    Cuda(Arc&lt;CudaGpu&gt;),
}
</code></pre>
<p>This represents either a Cpu or CudaGpu (potentially more in the future). A Device can be created from a Cpu or CudaGpu using the From trait:</p>
<pre><code>let cpu = Device::from(Cpu::new());
let gpu = Device::from(CudaGpu::new(0)); // Get the first gpu
</code></pre>
<p>Device also implements Default, which can be used to get a device, potentially the first gpu if available:</p>
<pre><code>let device = Device::default();
</code></pre>
<p>Device can be cloned, which clones the internal Arc (a threadsafe shared pointer). Devices cloned this way refer to the same object. The Cpu and CudaGpu structs store handles used to execute operations on with the backends (oneDNN and cuDNN for example).</p>
<h2><a class="header" href="#async" id="async">Async</a></h2>
<p>GPU operations are typically asynchronous with respect to the host. Allocations and transfers typically block the host thread, while operations like convolutions are simply enqueued and do not block. In a typical pattern, data is copied to the gpu, the operations are carried out, and the output is copied back to the host. No explicit synchronization is necessary, in fact the CUDA runtime synchronizes as needed. To explicity synchronize, and wait for all operations on the gpu to finish, call Device::synchronize(): </p>
<pre><code>let device = Device::from(CudaGpu::new());
let x = Tensor::&lt;u8&gt;::ones(&amp;device, 10)
    .to_one_hot_f32(16);
// wait for execution to finish
device.synchronize();
</code></pre>
<p>This can be useful for timing / benchmarking. </p>
<h1><a class="header" href="#num" id="num">Num</a></h1>
<p>Num is a trait for primitive datatypes that can be stored in a Tensor. More types will be added, but it can not be implemented outside the crate.</p>
<ul>
<li>f32 (32 bit float)</li>
<li>u8 (8 bit unsigned char)</li>
</ul>
<h2><a class="header" href="#unsigned" id="unsigned">Unsigned</a></h2>
<p>Unsigned is a subset of Num, for unsigned integral types, ie whole numbers. This is generally used to represent class labels.</p>
<ul>
<li>u8</li>
</ul>
<h1><a class="header" href="#tensorbase" id="tensorbase">TensorBase</a></h1>
<p>A tensor is a mathmatically generalization of a vector in n dimensions. Typically this means storing a vector of data and a vector of dimensions. </p>
<pre><code>#[derive(Clone)]
pub struct TensorBase&lt;S: Data, D: Dimension&gt; { ... }
</code></pre>
<p>Similar to ndarray's ArrayBase, TensorBase&lt;S, D&gt; is generic over the storage S and dimension D. S is required to implement the Data trait, which is sealed (not able to be implemented outside the crate):</p>
<pre><code>/// Main trait for Tensor S generic parameter, similar to ndarray::Data. Elem indicates that the Tensor stores that datatype.
pub trait Data: PrivateData {
    type Elem: Num;
}
</code></pre>
<p>The <a href="https://docs.rs/ndarray/0.13.1/ndarray/trait.Dimension.html">Dimension</a> trait looks like this:</p>
<pre><code>pub trait Dimension: Clone + Eq + Debug + Send + Sync + Default + IndexMut&lt;usize, Output = usize&gt; + Add&lt;Self, Output = Self&gt; + AddAssign + for&lt;'x&gt; AddAssign&lt;&amp;'x Self&gt; + Sub&lt;Self, Output = Self&gt; + SubAssign + for&lt;'x&gt; SubAssign&lt;&amp;'x Self&gt; + Mul&lt;usize, Output = Self&gt; + Mul&lt;Self, Output = Self&gt; + MulAssign + for&lt;'x&gt; MulAssign&lt;&amp;'x Self&gt; + MulAssign&lt;usize&gt; {
    type SliceArg: ?Sized + AsRef&lt;[SliceOrIndex]&gt;;
    type Pattern: IntoDimension&lt;Dim = Self&gt; + Clone + Debug + PartialEq + Eq + Default;
    type Smaller: Dimension;
    type Larger: Dimension + RemoveAxis;

    const NDIM: Option&lt;usize&gt;;

    fn ndim(&amp;self) -&gt; usize;
    fn into_pattern(self) -&gt; Self::Pattern;
    fn zeros(ndim: usize) -&gt; Self;
    fn __private__(&amp;self) -&gt; PrivateMarker;

    fn size(&amp;self) -&gt; usize { ... }
    fn size_checked(&amp;self) -&gt; Option&lt;usize&gt; { ... }
    fn as_array_view(&amp;self) -&gt; ArrayView1&lt;Ix&gt; { ... }
    fn as_array_view_mut(&amp;mut self) -&gt; ArrayViewMut1&lt;Ix&gt; { ... }
    fn into_dyn(self) -&gt; IxDyn { ... }
}
</code></pre>
<p>Dimensions:</p>
<ul>
<li>Ix0 (scalar)</li>
<li>Ix1 (vector)</li>
<li>Ix2 (matrix)</li>
<li>Ix3 (3d tensor)</li>
<li>Ix4 (4d tensor ie 2d image)</li>
<li>Ix5 (5d tensor ie 3d image)</li>
<li>Ix6 (6d tensor)</li>
<li>IxDyn (Nd tensor)</li>
</ul>
<p>Rather than constructing a Dimension directly, most functions in autograph use the <a href="https://docs.rs/ndarray/0.13.1/ndarray/trait.IntoDimension.html">IntoDimension</a> trait:</p>
<pre><code>pub trait IntoDimension {
    type Dim: Dimension;
    fn into_dimension(self) -&gt; Self::Dim;
}
</code></pre>
<p>This is implemented in ndarray for several types:</p>
<ul>
<li>usize -&gt; Ix0</li>
<li>(usize, usize) | [usize; 2] -&gt; Ix2</li>
<li>(usize, usize, usize) | [usize; 3] -&gt; Ix3</li>
<li>(usize, usize, usize, usize) | [usize; 4]` -&gt; Ix4</li>
<li>(usize, usize, usize, usize, usize) | [usize; 5]` -&gt; Ix5</li>
<li>(usize, usize, usize, usize, usize, usize) | [usize; 6] -&gt; Ix6</li>
<li>&amp;[usize] -&gt; IxDyn</li>
</ul>
<p>Similar to Array1, Array2, ArrayD, there are type aliases for Tensors as well, like Tensor1, Tensor2, TensorD, or TensorView1, TensorView2, TensorViewD etc.</p>
<h2><a class="header" href="#dataowned" id="dataowned">DataOwned</a></h2>
<p>The DataOwned trait is a subtrait of Data for tensors that own their data (ie without a borrow / lifetime). Implemented by:</p>
<ul>
<li>Tensor</li>
<li>ArcTensor</li>
<li>RwTensor</li>
</ul>
<p>This is used for all constructors, like:</p>
<pre><code>let x1 = Tensor::zeros(&amp;device, 1);
let x2 = ArcTensor::zeros(&amp;device, 1);
</code></pre>
<h3><a class="header" href="#cows" id="cows">Cows</a></h3>
<p><a href="https://doc.rust-lang.org/nightly/alloc/borrow/enum.Cow.html">Cow</a> (Copy On Write) is a &quot;Smart Pointer&quot; that is an enum, representing either Owned or Borrowed data. If the data is owned, it can be moved into the new container (ie a cpu Tensor). For gpu tensors, the data must be copied to gpu memory, so there is no advantage to passing an owned Vec over a slice. Often, data is not owned but borrowed. Instead of forcing the user to copy the data before calling the constructor, the user can provide the slice instead, allowing the implementation to choose when to copy the data, avoiding a second copy from the Vec to gpu memory. </p>
<p>The primary way to contruct a Tensor is with <code>TensorBase::from_shape_vec</code>:</p>
<pre><code>/// Constructs a Tensor on the device with the given shape. If a Vec is provided, will move the data (ie no copy) if the device is a cpu. Can also provide a slice, which allows for the data to only be copied once (rather than twice in the case of copying to the gpu).
    /// Panics: Asserts that the provided shape matches the length of the provided vec or slice.
    ///
pub fn from_shape_vec&lt;'a&gt;(
    device: &amp;Device,
    shape: impl IntoDimension&lt;Dim = D&gt;,
    vec: impl Into&lt;Cow&lt;'a, [T]&gt;&gt;,
) -&gt; Self;
</code></pre>
<p>Both <code>Vec&lt;T&gt;</code> and <code>&amp;[T]</code> implement <code>Into&lt;Cow&lt;[T]&gt;</code>, so this method can be called with a slice as well. The same is true for <code>TensorBase::from_array</code>:</p>
<pre><code>/// Similar to from_shape_vec, can accept either an Array or an ArrayView
    /// Copies that data to standard (packed) layout if necessary
pub fn from_array&lt;'a&gt;(device: &amp;Device, array: impl Into&lt;CowArray&lt;'a, T, D&gt;&gt;) -&gt; Self;
</code></pre>
<h2><a class="header" href="#dataref" id="dataref">DataRef</a></h2>
<p>The DataRef trait is a subtrait of Data for tensors that can borrow their data, ie they can return a TensorView. Implemented by:</p>
<ul>
<li>Tensor</li>
<li>ArcTensor</li>
<li>TensorView</li>
<li>TensorViewMut</li>
<li>RwReadTensor</li>
<li>RwWriteTensor</li>
</ul>
<p>Generally tensor operations will require their inputs to be DataRef. This ensures that the data is not modified while executing. </p>
<h2><a class="header" href="#datamut" id="datamut">DataMut</a></h2>
<p>Like DataRef, but allows mutable borrows. Implemented by:</p>
<ul>
<li>Tensor</li>
<li>TensorViewMut</li>
<li>RwWriteTensor</li>
</ul>
<h1><a class="header" href="#tensor" id="tensor">Tensor</a></h1>
<pre><code>pub type Tensor&lt;T, D&gt; = TensorBase&lt;OwnedRepr&lt;T&gt;, D&gt;;
</code></pre>
<p>A Tensor owns its data exclusively. Most tensor operations that return an output will return a Tensor. When a tensor is cloned, its data will be copied into a new tensor. </p>
<h1><a class="header" href="#tensorview" id="tensorview">TensorView</a></h1>
<pre><code>pub type TensorView&lt;'a, T, D&gt; = TensorBase&lt;ViewRepr&lt;&amp;'a Buffer&lt;T&gt;&gt;, D&gt;;
</code></pre>
<p>A TensorView is like a &amp;<code>[T]</code>, it represents a borrowed tensor. Tensors with S: DataRef can be borrowed as views with the view() method. </p>
<h1><a class="header" href="#tensorviewmut" id="tensorviewmut">TensorViewMut</a></h1>
<pre><code>pub type TensorViewMut&lt;'a, T, D&gt; = TensorBase&lt;ViewRepr&lt;&amp;'a mut Buffer&lt;T&gt;&gt;, D&gt;;
</code></pre>
<p>A TensorViewMut is lke a &amp;<code>[T]</code>, it represents a mutably borrowed tensor. Tensors with S: DataMut can be borrowed as mutable views with the view_mut() method.</p>
<pre><code></code></pre>
<h1><a class="header" href="#arctensor" id="arctensor">ArcTensor</a></h1>
<pre><code>pub type ArcTensor&lt;T, D&gt; = TensorBase&lt;ArcRepr&lt;T&gt;, D&gt;;
</code></pre>
<p>An ArcTensor is like Arc&lt;Vec<T>&gt;. Cloning an ArcTensor copies the pointer, sharing the data. The data is dropped when the last ArcTensor pointing to it is dropped.</p>
<h1><a class="header" href="#rwtensor" id="rwtensor">RwTensor</a></h1>
<pre><code>pub type RwTensor&lt;T, D&gt; = TensorBase&lt;RwRepr&lt;T&gt;, D&gt;;
</code></pre>
<p>A RwTensor is like an Arc&lt;RwLock&lt;Vec<T>&gt;&gt;. Cloning the RwTensor copies the pointer, like ArcTensor. Unlike ArcTensor, the data is wrapped in a RwLock. This allows either shared immutable access with the read() method (returning a RwReadTensor), or exclusive access with the write() method (returning a RwWriteTensor). </p>
<h2><a class="header" href="#rwreadtensor" id="rwreadtensor">RwReadTensor</a></h2>
<pre><code>pub type RwReadTensor&lt;'a, T, D&gt; = TensorBase&lt;RwReadRepr&lt;'a, T&gt;, D&gt;;
</code></pre>
<p>A RwReadTensor is like a RwLockReadGuard&lt;Vec<T>&gt;. On creation, the RwLock is locked in read mode, which blocks until a writer has released the lock, and then blocks any writers from obtaining the lock. A RwReadTensor can be borrowed as a TensorView with the view() method, and many methods are implemented generically over both. This is safe because the view will borrow the RwReadTensor, which holds the lock.</p>
<h2><a class="header" href="#rwwwritetensor" id="rwwwritetensor">RwWWriteTensor</a></h2>
<pre><code>pub type RwWriteTensor&lt;'a, T, D&gt; = TensorBase&lt;RwWriteRepr&lt;'a, T&gt;, D&gt;;
</code></pre>
<p>A RwReadTensor is like a RwLockWriteGuard&lt;Vec<T>&gt;. On creation, the RwLock is locked in write mode, which blocks until all readers and writers have released the lock, and then blocks any readers or writers from obtaining the lock. A RwWriteTensor can be borrowed as a TensorViewMut with the view_mut() method, and many methods are implemented generically over both. This is safe because the view will borrow the RwWriteTensor, which holds the lock.</p>
<h1><a class="header" href="#nn" id="nn">nn</a></h1>
<p>The nn module (short for Neural Network). </p>
<h2><a class="header" href="#api-hierarchy" id="api-hierarchy">API Hierarchy</a></h2>
<ul>
<li>Layers (Dense, Conv2d, MaxPool2d, etc)</li>
<li>Variable (ie functional interface)</li>
<li>Tensor (Variable methods call Tensor methods, much of this is private)</li>
<li>Device specific implementations on Tensor (private)</li>
</ul>
<h1><a class="header" href="#layers" id="layers">Layers</a></h1>
<p>It is common to compose neural networks with layers. Layers represent functions, but may also store both parameters like weights, and hyper parameters like the padding of a convolution. Layers make it easy to define a model, as they translate hyper parameters into the shape of the parameters. For example, you could create a simple Dense layer with this:</p>
<pre><code>let w = Parameter::new(
    ArcTensor::zeros(&amp;device, `[outputs, inputs]`)
);
let b = Parameter::new(
    ArcTensor::zeros(&amp;device, outputs)
);

// forward 
let y = x.dense(&amp;w, Some(&amp;b));
</code></pre>
<p>Or you could use a Dense layer:</p>
<pre><code>let dense = Dense::builder()
    .device(&amp;device)
    .inputs(inputs)
    .outputs(outputs)
    .build();
    
/// forward
let y = x.forward(&amp;dense);
</code></pre>
<h2><a class="header" href="#layer-trait" id="layer-trait">Layer Trait</a></h2>
<pre><code>/// Trait for Layers\
/// Custom Models should impl Layer
pub trait Layer {
    /// Returns a Vec of all the parameters in the Layer (including its children). Parameter acts like an Arc so it can be cloned to copy references. Layers that do not have parameters (like Activations) do not have to implement this method.
    fn parameters(&amp;self) -&gt; Vec&lt;ParameterD&gt; {
        Vec::new()
    }
    /// Prepares the layer for training if training is true, else prepares for evaluation / inference. This method should be called prior to a forward step ie:
    ///```
    /// for data in training_set {
    ///   let graph = Graph::new();
    ///   let (x, t) = // data
    ///   model.set_training(true);
    ///   let y = model.forward(&amp;x);
    ///   let loss = // loss function
    ///   loss.backward(graph);
    ///   // update model
    /// }
    /// for data in evaluation_set {
    ///   let (x, t) = // data
    ///   model.set_training(false);
    ///   let y = model.forward(&amp;x);
    ///   let loss = // loss function
    /// }
    ///```
    /// The implementation should recursively call set_training on all of its child layers, and or all of its parameters.
    fn set_training(&amp;mut self, training: bool) {}
}
</code></pre>
<p>The Layer trait provides a minimal interface for accessing parameters and setting the training mode. It is similar to PyTorch's Module. Layers can be composed in a struct:</p>
<pre><code>struct MyModel {
    layer1: Layer1,
    layer2: Layer2,
    // etc... 
}

// Note: you can also use a tuple struct, with unnamed fields
struct MyModel (
    Layer1, // my_model.0
    Layer2, // my_model.1
    // etc... 
);
</code></pre>
<p>There is a derive macro that will generate a Layer implementation for a struct composed of other layers:</p>
<pre><code>#[derive(Layer)]
struct MyModel { /*fields*/ }
</code></pre>
<h1><a class="header" href="#forward" id="forward">Forward</a></h1>
<pre><code>/// Trait for forward pass, implemented by layers\
/// Typically this will call a method or custom Trait method on Variable\
/// A layer like Conv2d will implement Forward, and a model composed of layers will also implement forward.
pub trait Forward&lt;D: Dimension&gt; {
    type OutputDim: Dimension;
    fn forward(&amp;self, input: &amp;Variable&lt;D&gt;) -&gt; Variable&lt;Self::OutputDim&gt;;
}
</code></pre>
<p>The Forward trait is used to compute the output given the input. When training, backward ops are enqueued into the graph attached to the input. The Layer and Forward traits combine to form the common interface of all layers / models. </p>
<h1><a class="header" href="#implementing-forward" id="implementing-forward">Implementing Forward</a></h1>
<p>Implementing forward for a struct is straightforward. For an image classifier, the input will be a Variable4 (NCHW), with D = Ix4, and the output will be Variable2 (NC). Use Variable's forward() method to sequence several methods, like this:</p>
<pre><code>struct ConvNet {
    conv: Conv2d,
    dense: Dense
}

impl Forward&lt;Ix4&gt; for ConvNet { // 
    type OutputDim = Ix2;
    fn forward(&amp;self, input: &amp;Variable4) -&gt; Variable2 {
        input
            .forward(&amp;self.conv)
            .relu()
            .flatten()
            .forward(&amp;self.dense)
    }
}
</code></pre>
<p>Similar to how Layer can be derived, Forward can be implemented with the impl_forward macro. We have to add the Relu and Flatten layers to our struct:</p>
<pre><code>#[impl_forward(Ix4, Ix2)]
#[derive(Layer)]
struct ConvNet (
    Conv2d,
    Relu,
    Dense,
    Flatten
);
</code></pre>
<p>Note that Rust has zero sized types. Relu and Flatten have no cost, they will only affect the derived implementations, not the size of the struct. Now you only have to write a constructor for the model, potentially something like this:</p>
<pre><code>impl ConvNet {
    pub fn new(device: &amp;Device) -&gt; Self {
        let conv = Conv2d::builder()
            .device(device)
            .inputs(1)
            .outputs(8)
            .args(
                Conv2dArgs::default()
                    .kernel(7)
            )
            .build();
        let relu = Relu::default();
        let flatten = Flatten::default();
        let Dense = Dense::builder()
            .inputs(8*22*22)
            .outputs(10)
            .bias()
            .build();
        ConvNet(
            conv,
            relu,
            flatten,
            dense
        ) 
    }
}
</code></pre>
<p>When developing it may be useful to experiment with several models. You can hide the type of the model in a few ways:</p>
<pre><code>// some models, with a new method like above
struct Model1 { .. }
struct Model2 { .. }

// define a type alias for use in training code
type MyModel = ConvNet; // ConvNet2

fn train() {
    let device = Device::default();
    let model = MyModel::new(&amp;device);
}
</code></pre>
<pre><code>// use a function that returns impl Trait

fn create_model(device: &amp;Device) -&gt; impl Layer + Forward&lt;Ix4, Output=Ix2&gt; {
    Model1::new(device) // Model2::new(device)
}

fn train() {
    let device = Device::default();
    let model = create_model();
}
</code></pre>
<p>Use a boxed dyn Trait:</p>
<pre><code>#[impl_forward(Ix4, Ix2)]
#[derive(Layer)]
struct DynModel(Box&lt;dyn (Layer + Forward&lt;Ix4, OutputDim=Ix2&gt;)&gt;);

impl DynModel {
    fn new(device: &amp;Device) -&gt; Self {
        let model = Model1::new(device) // Model2::new(device)
        DynModel(Box::new(model))
    }
}
</code></pre>
<h1><a class="header" href="#forward-1" id="forward-1">Forward</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
