<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>autograph</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="what-is-machine-learning.html"><strong aria-hidden="true">1.1.</strong> What is Machine Learning?</a></li><li class="chapter-item expanded "><a href="what-is-a-neural-network.html"><strong aria-hidden="true">1.2.</strong> What is a Neural Network?</a></li></ol></li><li class="chapter-item expanded "><a href="devices.html"><strong aria-hidden="true">2.</strong> Devices</a></li><li class="chapter-item expanded "><a href="num.html"><strong aria-hidden="true">3.</strong> Num</a></li><li class="chapter-item expanded "><a href="tensor-base.html"><strong aria-hidden="true">4.</strong> TensorBase</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tensor.html"><strong aria-hidden="true">4.1.</strong> Tensor</a></li><li class="chapter-item expanded "><a href="tensor-view.html"><strong aria-hidden="true">4.2.</strong> TensorView</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">autograph</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>The goal of autograph is to provide a Rust platform for Machine Learning. It taps into highly optimized, hardware specific libraries (written in c / c++ / assembly), but attempts to remain as agnostic as possible in regards to implementation. Operations can be implemented in Rust, c++, or any other language without direct dependence on each other, making it highly modular. 
Unlike many Machine Learning libraries, autograph is imperative (the natural programming paradigm). This is similar to PyTorch, where operations are called and executed immediately. One alternative is to use a symbolic approach, constructing a plan and repeatedly running it over and over with different data. While the later offers theoretical optimization benefits, it is more restrictive, less intuitive, and may make troublshooting more difficult. An imperative approach allows the insertion of arbitrary print statements for debugging, allows for loops, conditionals, and any other native operations. 
One idea at the core of Rust is mutability. In most c based languages, variables are mutable by default. In Rust, variables are immutable by default, and much of the language is designed around providng statically checked means of borrowing data (mutably or immutably) between functions. Inspired by the amazing ndarray crate, autograph emulates ArrayBase with TensorBase. Operations explicitly require immutable access to their inputs and mutable access to their outputs. For shared data, there is ArcTensor, an immutable shared tensor used for Inputs / Outputs, and RwTensor, which uses a RwLock to ensure thread safe shared mutability, used for Parameters and Gradients. 
In general, structures in autograph impl Send + Sync, and can be passed safely between threads. Underneath, there is plenty of unsafe used to wrap backend implementations, but the library can be used without writing unsafe code. </p>
<h1><a class="header" href="#what-is-machine-learning" id="what-is-machine-learning">What is Machine Learning?</a></h1>
<p>Machine Learning refers to algorithms that solve problems by iteratively improving until they converge to a solution. Unlike handcrafted logic, such algorithms scale to &quot;Big Data&quot;, potentially with billions of individually tuned parameters. Typically, such models separate datapoints into several classes, known as classification. This may be supervised, which means that some of the datapoints have been labeled by hand, or unsupervised, where the algorithm is not provided labeled datapoints. </p>
<h1><a class="header" href="#what-is-a-neural-network" id="what-is-a-neural-network">What is a Neural Network?</a></h1>
<p>Often referred to as &quot;Deep Learning&quot;, Neural Networks are composed of one or more layers of &quot;Neurons&quot;, that map some input x to some output y. Typically this is 'y = a(x*w + b)', where y, x, and w are matrices, b is a vector, and a is an &quot;Activation Function&quot;. An activation function is nonlinear, this helps the model approximate the dataset. </p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png?raw=true" alt="" /><br />
<em><a href="https://commons.wikimedia.org/wiki/File:Artificial_neural_network.svg">Artificial_neural_network</a></em></p>
<p>In the above image each vertical column of neurons represents a single 'y = a(x*w + b)', where there is a weight entry in w for each input entry in x. A neuron can be thought of a weighted some of the inputs plus a bias and mapped with an activation. The actual implementation uses matrices, with a batch of inputs, which is more efficient. A Neural Network is a supervised algorithm, it is trained by providing it with a target (ie the correct output). A loss function computes the error of the model output agaist the target. The goal of the model is to minimize this loss, which corresponds to approximating the dataset. In order to iteratively improve the prediction y, the parameters (w and b) are updated. This is done via &quot;Gradient Descent&quot;. All functions used to compute the loss are differentiable with respect to either their inputs and or their parameters. These gradients can be propagated backward, from the loss to the parameters. The parameters and then updated with some form of 'w -= lr * dw', where lr is the &quot;learning_rate&quot;, and dw is the gradient of the weight. The same is applied to the bias. </p>
<p>Prior to training, the weights of the model are initialized, perhaps from a normal distribution, while the biases are typically initialized as zeros. Input data is batched and fed into the model as a matrix, or a higher dimensional tensor (in the case of images). Larger batches are more efficient to compute than individual samples. The &quot;Forward Pass&quot; is computing the prediction of the model y and the loss. The &quot;Backward Pass&quot; is the computation of the gradients of the parameters. For each forward operation, there are one or more backward ops that compute the gradients of the inputs / parameters given the gradient of the output. This is the backward graph. Typically the backward ops are executed in First In Last Out order, ie in reverse. The forward, backward, and update steps are repeated for all of the training batches, this is an epoch. Training typically requires many epochs for model to converge to an accurate solution. </p>
<h1><a class="header" href="#devices" id="devices">Devices</a></h1>
<p>A Device represents either a cpu or gpu. Generally the gpu has its own memory. Transferring data from cpu to gpu memory is relatively expensive compared to executing operations. Generally operations can only be performed on data that is in a particular device's memory, at least multi device operations are slower than single device operations. </p>
<p>In autograph, a Device is an enum:</p>
<pre><code>#[derive(Clone, Debug)]
pub enum Device {
    Cpu(Arc&lt;Cpu&gt;),
    #[cfg(feature = &quot;cuda&quot;)]
    Cuda(Arc&lt;CudaGpu&gt;),
}
</code></pre>
<p>This represents either a Cpu or CudaGpu (potentially more in the future). A Device can be created from a Cpu or CudaGpu using the From trait:</p>
<pre><code>let cpu = Device::from(Cpu::new());
let gpu = Device::from(CudaGpu::new(0)); // Get the first gpu
</code></pre>
<p>Device also implements Default, which can be used to get a device, potentially the first gpu if available:</p>
<pre><code>let device = Device::default();
</code></pre>
<p>Device can be cloned, which clones the internal Arc (a threadsafe shared pointer). Devices cloned this way refer to the same object. The Cpu and CudaGpu structs store handles used to execute operations on with the backends (oneDNN and cuDNN for example).</p>
<h1><a class="header" href="#num" id="num">Num</a></h1>
<p>Num is a trait for primitive datatypes that can be stored in a Tensor. More types will be added, but it can not be implemented outside the crate.</p>
<ul>
<li>f32 (32 bit float)</li>
<li>u8 (8 bit unsigned char)</li>
</ul>
<h2><a class="header" href="#unsigned" id="unsigned">Unsigned</a></h2>
<p>Unsigned is a subset of Num, for unsigned integral types, ie whole numbers. This is generally used to represent class labels.</p>
<ul>
<li>u8</li>
</ul>
<h1><a class="header" href="#tensorbase" id="tensorbase">TensorBase</a></h1>
<p>A tensor is a mathmatically generalization of a vector in n dimensions. Typically this means storing a vector of data and a vector of dimensions. </p>
<pre><code>#[derive(Clone)]
pub struct TensorBase&lt;S: Data, D: Dimension&gt; { ... }
</code></pre>
<p>Similar to ndarray's ArrayBase, TensorBase&lt;S, D&gt; is generic over the storage S and dimension D. S is required to implement the Data trait, which is sealed (not able to be implemented outside the crate):</p>
<pre><code>/// Main trait for Tensor S generic parameter, similar to ndarray::Data. Elem indicates that the Tensor stores that datatype.
pub trait Data: PrivateData {
    type Elem: Num;
}
</code></pre>
<p>The <a href="https://docs.rs/ndarray/0.13.1/ndarray/trait.Dimension.html">Dimension</a> trait looks like this:</p>
<pre><code>pub trait Dimension: Clone + Eq + Debug + Send + Sync + Default + IndexMut&lt;usize, Output = usize&gt; + Add&lt;Self, Output = Self&gt; + AddAssign + for&lt;'x&gt; AddAssign&lt;&amp;'x Self&gt; + Sub&lt;Self, Output = Self&gt; + SubAssign + for&lt;'x&gt; SubAssign&lt;&amp;'x Self&gt; + Mul&lt;usize, Output = Self&gt; + Mul&lt;Self, Output = Self&gt; + MulAssign + for&lt;'x&gt; MulAssign&lt;&amp;'x Self&gt; + MulAssign&lt;usize&gt; {
    type SliceArg: ?Sized + AsRef&lt;[SliceOrIndex]&gt;;
    type Pattern: IntoDimension&lt;Dim = Self&gt; + Clone + Debug + PartialEq + Eq + Default;
    type Smaller: Dimension;
    type Larger: Dimension + RemoveAxis;

    const NDIM: Option&lt;usize&gt;;

    fn ndim(&amp;self) -&gt; usize;
    fn into_pattern(self) -&gt; Self::Pattern;
    fn zeros(ndim: usize) -&gt; Self;
    fn __private__(&amp;self) -&gt; PrivateMarker;

    fn size(&amp;self) -&gt; usize { ... }
    fn size_checked(&amp;self) -&gt; Option&lt;usize&gt; { ... }
    fn as_array_view(&amp;self) -&gt; ArrayView1&lt;Ix&gt; { ... }
    fn as_array_view_mut(&amp;mut self) -&gt; ArrayViewMut1&lt;Ix&gt; { ... }
    fn into_dyn(self) -&gt; IxDyn { ... }
}
</code></pre>
<p>Dimensions:
- Ix0 (scalar)
- Ix1 (vector)
- Ix2 (matrix)
- Ix3 (3d tensor)
- Ix4 (4d tensor ie 2d image)
- Ix5 (5d tensor ie 3d image)
- Ix6 (6d tensor)
- IxDyn (Nd tensor)</p>
<pre><code>Rather than constructing a Dimension directly, most functions in autograph use the [IntoDimension](https://docs.rs/ndarray/0.13.1/ndarray/trait.IntoDimension.html) trait:
</code></pre>
<p>pub trait IntoDimension {
type Dim: Dimension;
fn into_dimension(self) -&gt; Self::Dim;
}</p>
<pre><code>This is implemented in ndarray for several types:
    - usize -&gt; Ix0
    - (usize, usize) | `[usize; 2]` -&gt; Ix2
    - (usize, usize, usize) | `[usize; 3]` -&gt; Ix3
    - (usize, usize, usize, usize) | `[usize; 4]` -&gt; Ix4
    - (usize, usize, usize, usize, usize) | `[usize; 5]` -&gt; Ix5
    - (usize, usize, usize, usize, usize, usize) | `[usize; 6]` -&gt; Ix6
    - &amp;`[usize]` -&gt; IxDyn
</code></pre>
<p>Similar to Array1, Array2, ArrayD, there are type aliases for Tensors as well, like Tensor1, Tensor2, TensorD, or TensorView1, TensorView2, TensorViewD etc.</p>
<h2><a class="header" href="#dataowned" id="dataowned">DataOwned</a></h2>
<p>The DataOwned trait is a subtrait of Data for tensors that own their data (ie without a borrow / lifetime). Implemented by:
- Tensor
- ArcTensor
- RwTensor</p>
<p>This is used for all constructors, like:</p>
<pre><code>let x1 = Tensor::zeros(&amp;device, 1);
let x2 = ArcTensor::zeros(&amp;device, 1);
</code></pre>
<h3><a class="header" href="#cows" id="cows">Cows</a></h3>
<p><a href="https://doc.rust-lang.org/nightly/alloc/borrow/enum.Cow.html">Cow</a> (Copy On Write) is a &quot;Smart Pointer&quot; that is an enum, representing either Owned or Borrowed data. If the data is owned, it can be moved into the new container (ie a cpu Tensor). For gpu tensors, the data must be copied to gpu memory, so there is no advantage to passing an owned Vec over a slice. Often, data is not owned but borrowed. Instead of forcing the user to copy the data before calling the constructor, the user can provide the slice instead, allowing the implementation to choose when to copy the data, avoiding a second copy from the Vec to gpu memory. </p>
<p>The primary way to contruct a Tensor is with <code>TensorBase::from_shape_vec</code>:</p>
<pre><code>/// Constructs a Tensor on the device with the given shape. If a Vec is provided, will move the data (ie no copy) if the device is a cpu. Can also provide a slice, which allows for the data to only be copied once (rather than twice in the case of copying to the gpu).
    /// Panics: Asserts that the provided shape matches the length of the provided vec or slice.
    ///
pub fn from_shape_vec&lt;'a&gt;(
    device: &amp;Device,
    shape: impl IntoDimension&lt;Dim = D&gt;,
    vec: impl Into&lt;Cow&lt;'a, [T]&gt;&gt;,
) -&gt; Self;
</code></pre>
<p>Both <code>Vec&lt;T&gt;</code> and <code>&amp;[T]</code> implement <code>Into&lt;Cow&lt;[T]&gt;</code>, so this method can be called with a slice as well. The same is true for <code>TensorBase::from_array</code>:</p>
<pre><code>/// Similar to from_shape_vec, can accept either an Array or an ArrayView
    /// Copies that data to standard (packed) layout if necessary
pub fn from_array&lt;'a&gt;(device: &amp;Device, array: impl Into&lt;CowArray&lt;'a, T, D&gt;&gt;) -&gt; Self;
</code></pre>
<h2><a class="header" href="#dataref" id="dataref">DataRef</a></h2>
<p>The DataRef trait is a subtrait of Data for tensors that can borrow their data, ie they can return a TensorView. Implemented by:
- Tensor
- ArcTensor
- TensorView
- TensorViewMut
- RwReadTensor
- RwWriteTensor</p>
<p>Generally tensor operations will require their inputs to be DataRef. This ensures that the data is not modified while executing. </p>
<h2><a class="header" href="#datamut" id="datamut">DataMut</a></h2>
<p>Like DataRef, but allows mutable borrows. Implemented by:
- Tensor
- TensorViewMut
- RwWriteTensor</p>
<h1><a class="header" href="#tensor" id="tensor">Tensor</a></h1>
<pre><code>pub type Tensor&lt;T, D&gt; = TensorBase&lt;OwnedRepr&lt;T&gt;, D&gt;;
</code></pre>
<p>A Tensor owns its data exclusively. Most tensor operations that return an output will return a Tensor. When a tensor is cloned, its data will be copied into a new tensor. </p>
<h1><a class="header" href="#tensorview" id="tensorview">TensorView</a></h1>
<pre><code>pub type TensorView&lt;'a, T, D&gt; = TensorBase&lt;ViewRepr&lt;&amp;'a Buffer&lt;T&gt;&gt;, D&gt;;
</code></pre>
<p>A TensorView is like a &amp;<code>[T]</code>, it represents a borrowed tensor. Tensors with S: DataRef can be borrowed as views.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
